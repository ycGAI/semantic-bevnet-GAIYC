#!/usr/bin/env python3
"""
MCAP to KITTI Odometry Dataset Converter with Multiprocessing
Converts ROS2 bag data to KITTI Odometry format with synchronized timestamps
"""

import argparse
import os
import cv2
import numpy as np
from multiprocessing import Pool, Manager, cpu_count
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
import time
from datetime import datetime
from scipy.spatial.transform import Rotation as R
from rclpy.serialization import deserialize_message
from rosidl_runtime_py.utilities import get_message
import rosbag2_py
from tqdm import tqdm
import struct

class MCAPToKITTIOdometryConverter:
    def __init__(self, input_bag, output_dir, sequence_id="00", num_processes=None):
        self.input_bag = input_bag
        self.output_dir = Path(output_dir)
        self.sequence_id = sequence_id
        self.sequence_dir = self.output_dir / "sequences" / sequence_id
        self.num_processes = num_processes or max(1, cpu_count() - 1)
        
        # KITTI Odometryç›®å½•ç»“æ„
        self.setup_directories()
        
        # æ•°æ®å­˜å‚¨
        self.poses_data = []
        self.camera_data = []
        self.lidar_data = []
        self.timestamps = []
        
    def setup_directories(self):
        """åˆ›å»ºKITTI Odometryæ ‡å‡†ç›®å½•ç»“æ„"""
        directories = [
            f'sequences/{self.sequence_id}/image_0',   # å·¦ç›¸æœºï¼ˆç°åº¦ï¼‰
            f'sequences/{self.sequence_id}/image_1',   # å³ç›¸æœºï¼ˆç°åº¦ï¼‰
            f'sequences/{self.sequence_id}/image_2',   # å·¦ç›¸æœºï¼ˆå½©è‰²ï¼‰
            f'sequences/{self.sequence_id}/image_3',   # å³ç›¸æœºï¼ˆå½©è‰²ï¼‰
            f'sequences/{self.sequence_id}/velodyne',  # æ¿€å…‰é›·è¾¾ç‚¹äº‘
            f'sequences/{self.sequence_id}/labels',    # è¯­ä¹‰åˆ†å‰²æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
            'poses'  # ä½å§¿æ–‡ä»¶ç›®å½•
        ]
        
        for dir_name in directories:
            (self.output_dir / dir_name).mkdir(parents=True, exist_ok=True)
        
        print(f"âœ… åˆ›å»ºKITTI Odometryç›®å½•ç»“æ„: {self.output_dir}")
        print(f"ğŸ“ åºåˆ—ç›®å½•: {self.sequence_dir}")

    def read_bag_messages(self):
        """è¯»å–bagæ–‡ä»¶ä¸­çš„æ‰€æœ‰æ¶ˆæ¯"""
        reader = rosbag2_py.SequentialReader()
        reader.open(
            rosbag2_py.StorageOptions(uri=self.input_bag, storage_id="mcap"),
            rosbag2_py.ConverterOptions(
                input_serialization_format="cdr", 
                output_serialization_format="cdr"
            ),
        )

        topic_types = reader.get_all_topics_and_types()
        type_map = {topic.name: topic.type for topic in topic_types}

        messages = []
        print("ğŸ“– è¯»å–bagæ–‡ä»¶æ¶ˆæ¯...")
        
        while reader.has_next():
            topic, data, timestamp = reader.read_next()
            messages.append((topic, data, timestamp, type_map.get(topic)))
        
        del reader
        print(f"âœ… è¯»å–å®Œæˆï¼Œå…± {len(messages)} æ¡æ¶ˆæ¯")
        return messages

    def process_message_batch(self, message_batch):
        """å¤„ç†ä¸€æ‰¹æ¶ˆæ¯ï¼ˆå¤šè¿›ç¨‹è°ƒç”¨ï¼‰"""
        poses = []
        cameras = []
        lidars = []
        timestamps = []
        
        for topic, data, timestamp, msg_type_name in message_batch:
            if not msg_type_name:
                continue
                
            try:
                msg_type = get_message(msg_type_name)
                msg = deserialize_message(data, msg_type)
                
                # å¤„ç†é‡Œç¨‹è®¡æ•°æ®ï¼ˆposesï¼‰
                if topic == '/odometry/filtered/global':
                    pose_data = self.extract_pose_data(msg, timestamp)
                    if pose_data:
                        poses.append(pose_data)
                        timestamps.append(timestamp)
                
                # å¤„ç†ç›¸æœºæ•°æ®
                elif any(keyword in topic.lower() for keyword in ['image', 'camera']):
                    camera_data = self.extract_camera_data(msg, timestamp, topic)
                    if camera_data:
                        cameras.append(camera_data)
                        if timestamp not in timestamps:
                            timestamps.append(timestamp)
                
                # å¤„ç†æ¿€å…‰é›·è¾¾æ•°æ®
                elif any(keyword in topic.lower() for keyword in ['velodyne', 'lidar', 'pointcloud', 'points']):
                    lidar_data = self.extract_lidar_data(msg, timestamp)
                    if lidar_data:
                        lidars.append(lidar_data)
                        if timestamp not in timestamps:
                            timestamps.append(timestamp)
                        
            except Exception as e:
                print(f"âš ï¸  å¤„ç†æ¶ˆæ¯å¤±è´¥ {topic}: {e}")
                continue
        
        return poses, cameras, lidars, sorted(set(timestamps))

    def extract_pose_data(self, msg, timestamp):
        """æå–ä½å§¿æ•°æ®"""
        try:
            pos = msg.pose.pose.position
            rot = msg.pose.pose.orientation
            
            # å››å…ƒæ•°è½¬æ—‹è½¬çŸ©é˜µ
            rotation = R.from_quat([rot.x, rot.y, rot.z, rot.w])
            rot_matrix = rotation.as_matrix()
            
            # æ„å»º4x4å˜æ¢çŸ©é˜µ
            transform = np.eye(4)
            transform[:3, :3] = rot_matrix
            transform[:3, 3] = [pos.x, pos.y, pos.z]
            
            # KITTIæ ¼å¼ï¼šå‰3è¡Œå±•å¼€ä¸º12ä¸ªæ•°å€¼
            kitti_pose = transform[:3, :].flatten()
            
            return {
                'timestamp': timestamp,
                'pose': kitti_pose,
                'position': (pos.x, pos.y, pos.z),
                'orientation': (rot.x, rot.y, rot.z, rot.w)
            }
        except Exception as e:
            print(f"âš ï¸  æå–ä½å§¿æ•°æ®å¤±è´¥: {e}")
            return None

    def extract_camera_data(self, msg, timestamp, topic):
        """æå–ç›¸æœºæ•°æ®"""
        try:
            # å¤„ç†å‹ç¼©å›¾åƒæ ¼å¼ (CompressedImage)
            if hasattr(msg, 'format') and hasattr(msg, 'data'):
                return {
                    'timestamp': timestamp,
                    'topic': topic,
                    'format': msg.format,  # å‹ç¼©æ ¼å¼ (jpeg, pngç­‰)
                    'data': bytes(msg.data),
                    'is_compressed': True
                }
            # å¤„ç†åŸå§‹å›¾åƒæ ¼å¼ (Image)
            elif hasattr(msg, 'width') and hasattr(msg, 'height'):
                return {
                    'timestamp': timestamp,
                    'topic': topic,
                    'width': msg.width,
                    'height': msg.height,
                    'encoding': msg.encoding,
                    'data': bytes(msg.data),
                    'step': getattr(msg, 'step', msg.width * 3),
                    'is_compressed': False
                }
        except Exception as e:
            print(f"âš ï¸  æå–ç›¸æœºæ•°æ®å¤±è´¥: {e}")
            return None

    def extract_lidar_data(self, msg, timestamp):
        """æå–æ¿€å…‰é›·è¾¾æ•°æ®"""
        try:
            if hasattr(msg, 'data'):
                return {
                    'timestamp': timestamp,
                    'width': getattr(msg, 'width', 1),
                    'height': getattr(msg, 'height', len(msg.data) // msg.point_step if hasattr(msg, 'point_step') else 1),
                    'point_step': getattr(msg, 'point_step', 16),
                    'row_step': getattr(msg, 'row_step', len(msg.data)),
                    'data': bytes(msg.data),
                    'fields': [(f.name, f.offset, f.datatype, f.count) for f in msg.fields] if hasattr(msg, 'fields') else []
                }
        except Exception as e:
            print(f"âš ï¸  æå–æ¿€å…‰é›·è¾¾æ•°æ®å¤±è´¥: {e}")
            return None

    def save_camera_image(self, camera_data, index):
        """ä¿å­˜ç›¸æœºå›¾åƒåˆ°KITTI Odometryæ ¼å¼"""
        try:
            # å¤„ç†å‹ç¼©å›¾åƒ
            if camera_data.get('is_compressed', False):
                # è§£ç å‹ç¼©å›¾åƒ
                np_arr = np.frombuffer(camera_data['data'], np.uint8)
                img = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
                
                if img is None:
                    print(f"âš ï¸  æ— æ³•è§£ç å‹ç¼©å›¾åƒï¼Œæ ¼å¼: {camera_data.get('format', 'unknown')}")
                    return False
                
                # è½¬æ¢ä¸ºç°åº¦å’Œå½©è‰²
                if len(img.shape) == 3:
                    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                    color_img = img
                else:
                    gray_img = img
                    color_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
            
            # å¤„ç†åŸå§‹å›¾åƒ
            else:
                width = camera_data['width']
                height = camera_data['height']
                encoding = camera_data['encoding']
                data = camera_data['data']
                
                if encoding == 'bgr8':
                    img = np.frombuffer(data, np.uint8).reshape((height, width, 3))
                    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                    color_img = img
                elif encoding == 'rgb8':
                    img = np.frombuffer(data, np.uint8).reshape((height, width, 3))
                    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
                    color_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
                elif encoding == 'mono8':
                    gray_img = np.frombuffer(data, np.uint8).reshape((height, width))
                    color_img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)
                elif encoding == 'mono16':
                    img_16 = np.frombuffer(data, np.uint16).reshape((height, width))
                    gray_img = (img_16 / 256).astype(np.uint8)  # è½¬æ¢ä¸º8ä½
                    color_img = cv2.cvtColor(gray_img, cv2.COLOR_GRAY2BGR)
                else:
                    print(f"âš ï¸  ä¸æ”¯æŒçš„å›¾åƒç¼–ç : {encoding}")
                    return False
            
            # æ ¹æ®è¯é¢˜åç¡®å®šæ˜¯å·¦ç›¸æœºè¿˜æ˜¯å³ç›¸æœº
            is_right_camera = any(keyword in camera_data['topic'].lower() 
                                for keyword in ['right', 'image_01', 'image_1', 'cam1'])
            
            # ä¿å­˜ç°åº¦å›¾åƒï¼ˆimage_0 å’Œ image_1ï¼‰
            if is_right_camera:
                gray_path = self.sequence_dir / 'image_1' / f'{index:06d}.png'
                color_path = self.sequence_dir / 'image_3' / f'{index:06d}.png'
            else:
                gray_path = self.sequence_dir / 'image_0' / f'{index:06d}.png'
                color_path = self.sequence_dir / 'image_2' / f'{index:06d}.png'
            
            # ä¿å­˜å›¾åƒ
            cv2.imwrite(str(gray_path), gray_img)
            cv2.imwrite(str(color_path), color_img)
            
            return True
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜å›¾åƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    def save_lidar_points(self, lidar_data, index):
        """ä¿å­˜æ¿€å…‰é›·è¾¾ç‚¹äº‘ä¸ºKITTIæ ¼å¼"""
        try:
            data = lidar_data['data']
            point_step = lidar_data['point_step']
            fields = lidar_data['fields']
            
            # è§£æç‚¹äº‘å­—æ®µ
            field_map = {}
            for field_name, offset, datatype, count in fields:
                field_map[field_name] = (offset, datatype, count)
            
            # è®¡ç®—ç‚¹çš„æ•°é‡
            num_points = len(data) // point_step
            points = []
            
            for i in range(num_points):
                point_data = data[i * point_step:(i + 1) * point_step]
                
                # æå– x, y, z
                x = struct.unpack('f', point_data[0:4])[0] if len(point_data) >= 4 else 0.0
                y = struct.unpack('f', point_data[4:8])[0] if len(point_data) >= 8 else 0.0
                z = struct.unpack('f', point_data[8:12])[0] if len(point_data) >= 12 else 0.0
                
                # æå–å¼ºåº¦ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if 'intensity' in field_map and len(point_data) >= 16:
                    intensity = struct.unpack('f', point_data[12:16])[0]
                else:
                    intensity = 0.0
                
                points.append([x, y, z, intensity])
            
            if points:
                points_array = np.array(points, dtype=np.float32)
                
                # ä¿å­˜ä¸ºKITTIæ ¼å¼ï¼ˆäºŒè¿›åˆ¶ï¼ŒNÃ—4çš„float32æ•°ç»„ï¼‰
                output_path = self.sequence_dir / 'velodyne' / f'{index:06d}.bin'
                points_array.tofile(output_path)
                return True
            
            return False
            
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç‚¹äº‘å¤±è´¥: {e}")
            return False

    def save_poses(self):
        """ä¿å­˜ä½å§¿æ–‡ä»¶"""
        if not self.poses_data:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä½å§¿æ•°æ®")
            return
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        self.poses_data.sort(key=lambda x: x['timestamp'])
        
        # ä¿å­˜åˆ° poses/åºåˆ—å·.txt
        poses_file = self.output_dir / 'poses' / f'{self.sequence_id}.txt'
        poses_file.parent.mkdir(exist_ok=True)
        
        with open(poses_file, 'w') as f:
            for pose_data in self.poses_data:
                pose_str = ' '.join(f'{val:.6f}' for val in pose_data['pose'])
                f.write(pose_str + '\n')
        
        # åŒæ—¶åœ¨åºåˆ—ç›®å½•ä¸‹ä¹Ÿä¿å­˜ä¸€ä»½
        sequence_poses_file = self.sequence_dir / 'poses.txt'
        with open(sequence_poses_file, 'w') as f:
            for pose_data in self.poses_data:
                pose_str = ' '.join(f'{val:.6f}' for val in pose_data['pose'])
                f.write(pose_str + '\n')
        
        print(f"âœ… ä¿å­˜ {len(self.poses_data)} ä¸ªä½å§¿åˆ°:")
        print(f"   {poses_file}")
        print(f"   {sequence_poses_file}")

    def save_times(self):
        """ä¿å­˜æ—¶é—´æˆ³æ–‡ä»¶"""
        if not self.timestamps:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ—¶é—´æˆ³æ•°æ®")
            return
        
        # æ’åºå¹¶å»é‡
        unique_timestamps = sorted(set(self.timestamps))
        
        # ä¿å­˜åˆ° sequences/åºåˆ—å·/times.txt
        times_file = self.sequence_dir / 'times.txt'
        
        with open(times_file, 'w') as f:
            for timestamp in unique_timestamps:
                # è½¬æ¢çº³ç§’æ—¶é—´æˆ³ä¸ºç§’ï¼ˆæµ®ç‚¹æ•°ï¼‰
                timestamp_sec = timestamp / 1e9
                f.write(f'{timestamp_sec:.6f}\n')
        
        print(f"âœ… ä¿å­˜ {len(unique_timestamps)} ä¸ªæ—¶é—´æˆ³åˆ° {times_file}")

    def create_calib_file(self):
        """åˆ›å»ºæ ‡å®šæ–‡ä»¶"""
        calib_file = self.sequence_dir / 'calib.txt'
        
        # KITTIæ ‡å®šæ–‡ä»¶æ¨¡æ¿
        calib_content = """# KITTI Calibration File Template
# Please update these values with your actual calibration parameters

# Projection matrix for left gray camera (image_0)
P0: 7.215377e+02 0.000000e+00 6.095593e+02 0.000000e+00 0.000000e+00 7.215377e+02 1.728540e+02 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00

# Projection matrix for right gray camera (image_1)
P1: 7.215377e+02 0.000000e+00 6.095593e+02 -3.875744e+02 0.000000e+00 7.215377e+02 1.728540e+02 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00

# Projection matrix for left color camera (image_2)
P2: 7.215377e+02 0.000000e+00 6.095593e+02 0.000000e+00 0.000000e+00 7.215377e+02 1.728540e+02 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00

# Projection matrix for right color camera (image_3)
P3: 7.215377e+02 0.000000e+00 6.095593e+02 -3.875744e+02 0.000000e+00 7.215377e+02 1.728540e+02 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00

# Transformation from velodyne to left camera
Tr: 4.276802385584e-04 -9.999672484946e-01 -8.084491683471e-03 -1.198459927713e-02 -7.210626507497e-03 8.081198471645e-03 -9.999413164504e-01 -5.403984729748e-02 9.999738645903e-01 4.859485810390e-04 -7.206933692422e-03 -2.921968648686e-01
"""
        
        with open(calib_file, 'w') as f:
            f.write(calib_content)
        
        print(f"ğŸ“ åˆ›å»ºæ ‡å®šæ–‡ä»¶: {calib_file}")
        print("âš ï¸  è¯·æ ¹æ®å®é™…æƒ…å†µæ›´æ–°æ ‡å®šå‚æ•°ï¼")

    def convert(self):
        """ä¸»è½¬æ¢å‡½æ•°"""
        start_time = time.time()
        print(f"ğŸš€ å¼€å§‹è½¬æ¢ MCAP åˆ° KITTI Odometry æ ¼å¼")
        print(f"ğŸ“ è¾“å…¥: {self.input_bag}")
        print(f"ğŸ“ è¾“å‡º: {self.output_dir}")
        print(f"ğŸ”¢ åºåˆ—å·: {self.sequence_id}")
        print(f"âš™ï¸  ä½¿ç”¨ {self.num_processes} ä¸ªè¿›ç¨‹")
        
        # è¯»å–æ‰€æœ‰æ¶ˆæ¯
        all_messages = self.read_bag_messages()
        
        # å°†æ¶ˆæ¯åˆ†æ‰¹å¤„ç†
        batch_size = max(100, len(all_messages) // self.num_processes)
        message_batches = [
            all_messages[i:i+batch_size] 
            for i in range(0, len(all_messages), batch_size)
        ]
        
        print(f"ğŸ“¦ åˆ†ä¸º {len(message_batches)} æ‰¹å¤„ç†ï¼Œæ¯æ‰¹çº¦ {batch_size} æ¡æ¶ˆæ¯")
        
        # å¤šè¿›ç¨‹å¤„ç†æ¶ˆæ¯
        with Pool(self.num_processes) as pool:
            results = list(tqdm(
                pool.imap(self.process_message_batch, message_batches),
                total=len(message_batches),
                desc="å¤„ç†æ¶ˆæ¯"
            ))
        
        # åˆå¹¶ç»“æœ
        all_timestamps = set()
        for poses, cameras, lidars, timestamps in results:
            self.poses_data.extend(poses)
            self.camera_data.extend(cameras)
            self.lidar_data.extend(lidars)
            all_timestamps.update(timestamps)
        
        self.timestamps = list(all_timestamps)
        
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   ä½å§¿: {len(self.poses_data)}")
        print(f"   ç›¸æœº: {len(self.camera_data)}")
        print(f"   æ¿€å…‰é›·è¾¾: {len(self.lidar_data)}")
        print(f"   æ—¶é—´æˆ³: {len(self.timestamps)}")
        
        # æŒ‰æ—¶é—´æˆ³å¯¹æ‰€æœ‰æ•°æ®æ’åº
        self.camera_data.sort(key=lambda x: x['timestamp'])
        self.lidar_data.sort(key=lambda x: x['timestamp'])
        
        # ä¿å­˜æ•°æ®
        print("ğŸ’¾ ä¿å­˜æ•°æ®æ–‡ä»¶...")
        
        # ä¿å­˜ä½å§¿å’Œæ—¶é—´æˆ³
        self.save_poses()
        self.save_times()
        
        # å¤šçº¿ç¨‹ä¿å­˜å›¾åƒå’Œç‚¹äº‘
        with ThreadPoolExecutor(max_workers=self.num_processes) as executor:
            # ä¿å­˜ç›¸æœºå›¾åƒ
            camera_futures = []
            for i, camera_data in enumerate(self.camera_data):
                future = executor.submit(self.save_camera_image, camera_data, i)
                camera_futures.append(future)
            
            # ä¿å­˜æ¿€å…‰é›·è¾¾ç‚¹äº‘
            lidar_futures = []
            for i, lidar_data in enumerate(self.lidar_data):
                future = executor.submit(self.save_lidar_points, lidar_data, i)
                lidar_futures.append(future)
            
            # ç­‰å¾…å®Œæˆ
            if camera_futures:
                for future in tqdm(camera_futures, desc="ä¿å­˜å›¾åƒ"):
                    future.result()
            
            if lidar_futures:
                for future in tqdm(lidar_futures, desc="ä¿å­˜ç‚¹äº‘"):
                    future.result()
        
        # åˆ›å»ºæ ‡å®šæ–‡ä»¶
        self.create_calib_file()
        
        elapsed_time = time.time() - start_time
        print(f"âœ… è½¬æ¢å®Œæˆï¼ç”¨æ—¶ {elapsed_time:.2f} ç§’")
        print(f"ğŸ“ è¾“å‡ºç›®å½•ç»“æ„:")
        print(f"   {self.output_dir}/sequences/{self.sequence_id}/")
        print(f"   â”œâ”€â”€ image_0/      # å·¦ç›¸æœºç°åº¦å›¾")
        print(f"   â”œâ”€â”€ image_2/      # å·¦ç›¸æœºå½©è‰²å›¾")
        print(f"   â”œâ”€â”€ velodyne/     # æ¿€å…‰é›·è¾¾ç‚¹äº‘")
        print(f"   â”œâ”€â”€ poses.txt     # ä½å§¿æ–‡ä»¶")
        print(f"   â”œâ”€â”€ times.txt     # æ—¶é—´æˆ³æ–‡ä»¶")
        print(f"   â””â”€â”€ calib.txt     # æ ‡å®šæ–‡ä»¶")


def main():
    parser = argparse.ArgumentParser(
        description="Convert MCAP ROS2 bag to KITTI Odometry dataset format"
    )
    parser.add_argument("input", help="è¾“å…¥MCAPæ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", required=True, help="è¾“å‡ºKITTIæ•°æ®é›†ç›®å½•")
    parser.add_argument("-s", "--sequence", default="00", help="åºåˆ—å· (é»˜è®¤: 00)")
    parser.add_argument("-j", "--jobs", type=int, help="å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤ï¼šCPUæ ¸å¿ƒæ•°-1ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="ä»…åˆ†ææ•°æ®ï¼Œä¸æ‰§è¡Œè½¬æ¢")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        return
    
    if args.dry_run:
        print("ğŸ” ä»…åˆ†ææ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…è½¬æ¢")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åˆ†æä»£ç 
        return
    
    converter = MCAPToKITTIOdometryConverter(
        input_bag=args.input,
        output_dir=args.output,
        sequence_id=args.sequence,
        num_processes=args.jobs
    )
    
    try:
        converter.convert()
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­è½¬æ¢")
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
#ls -1 | tail -n +1001 | xargs -r rm -f